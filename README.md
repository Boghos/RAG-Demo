# ğŸ“š RAG Demo â€” LangChain + Gemini + Hugging Face + Chroma

This project demonstrates an **end-to-end Retrieval-Augmented Generation (RAG)** system built using **LangChain**, **Google Gemini (via `langchain_google_genai`)**, **Hugging Face embeddings**, and **ChromaDB**.

It loads a **PDF document**, splits it into chunks, stores them as embeddings in a Chroma vector database, and answers user questions using **Google Gemini 2.5 Flash Lite**, powered by retrieved context from the vector store.

---

## ğŸš€ Features

- ğŸ§  Uses **Google Gemini (Generative AI)** for reasoning and question answering.
- ğŸ” Uses **Hugging Face embeddings** via the **Hugging Face Inference API**.
- ğŸ—‚ï¸ Stores document embeddings in **ChromaDB**, a local vector database.
- ğŸ“„ Accepts any **PDF** file as input.
- ğŸ” Full **manual RAG pipeline** using LangChain's new **LCEL syntax** (no `RetrievalQA`).
- ğŸ’¬ Interactive command-line question answering loop.

---

## ğŸ§° Tech Stack

| Component                | Library                                        |
| ------------------------ | ---------------------------------------------- |
| **Language**             | Python 3.10+                                   |
| **Vector Store**         | [Chroma](https://docs.trychroma.com/)          |
| **Embeddings**           | HuggingFaceEndpointEmbeddings                  |
| **LLM**                  | ChatGoogleGenerativeAI (Gemini 2.5 Flash Lite) |
| **Document Loader**      | LangChain PyPDFLoader                          |
| **Prompting & Chaining** | LangChain Core (LCEL syntax)                   |
| **Env Management**       | python-dotenv                                  |

---

## ğŸ“¦ Installation

### 1ï¸âƒ£ Clone the Repository

```bash
git clone https://github.com/<your-username>/rag-demo.git
cd rag-demo
```

### 2ï¸âƒ£ Create a Virtual Environment

```bash
python -m venv venv
source venv/bin/activate  # Mac/Linux
venv\Scripts\activate     # Windows
```

### 3ï¸âƒ£ Install Dependencies

```bash
pip install -r requirements.txt
```

---

## ğŸ”‘ Environment Variables

Create a file named `.env` in the project root with:

```bash
GOOGLE_API_KEY=your_google_api_key_here
HUGGINGFACE_API_KEY=your_huggingface_api_key_here
```

âš ï¸ Do not share this file â€” your API keys are private. You can provide a safe `.env.example` for reference in GitHub.

You can get free API keys here:

- **Google Gemini** â†’ https://aistudio.google.com/
- **Hugging Face** â†’ https://huggingface.co/settings/tokens

---

## ğŸ“„ Usage

### 1ï¸âƒ£ Add your PDF file

Place your PDF file in a folder called `data/` and update this line in `app.py`:

```python
pdf_path = "data/your_file.pdf"
```

### 2ï¸âƒ£ Run the Application

```bash
python app.py
```

### 3ï¸âƒ£ Ask Questions

You'll enter an interactive mode like this:

```
Ask a question (or type 'exit'): What is this document about?
Answer: This PDF describes the motivation and goals of...
```

The app will also display the top retrieved source snippets used to answer.

---

## ğŸ§  How It Works

**Document Loading**

The PDF is loaded and split into small overlapping chunks.

**Embedding Creation**

Each chunk is embedded into a numerical vector using Hugging Face.

**Vector Storage**

The chunks and embeddings are stored in ChromaDB locally.

**Query Process**

The user's question is embedded and compared against stored vectors. The top-matching chunks are retrieved. Gemini uses these chunks as context to generate a grounded answer.

---

## ğŸ§ª Example Run

```
Ask a question (or type 'exit'): What inspired the author to write this letter?
Answer: The author was motivated by a strong interest in...

Sources (2 documents):
  1. I am writing to express my motivation...
  2. My background in computer science...
```

---

## âš™ï¸ Future Enhancements

- ğŸŒ Add a FastAPI or Streamlit frontend
- ğŸ§  Multi-document retrieval and ranking
- ğŸ’¾ Cache embeddings and allow re-indexing
- ğŸ§® Support for model parameter tuning
